{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation buffer\n",
        "\n",
        "Esta es una de las memorias m치s b치sicas, cada prompt y respuesta del modelos se almacenara en la memoria. Cada vez que se le envia un nuevo prompt al modelo se envia todo el historico de las interacciones.\n",
        "\n",
        "La conversaci칩n se salva como pares de mensajes entre \"Human\" y \"AI\", por lo cual tambien lo podemos integrar con modelos como GPT3.5 Turbo"
      ],
      "metadata": {
        "id": "OTbKMvTSr6hb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCeGaJ-rU_W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "ikugEi3Qr6VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI()\n",
        "memoria = ConversationBufferMemory()\n",
        "chatbot = ConversationChain(llm=llm, memory=memoria, verbose=True)"
      ],
      "metadata": {
        "id": "CNYZLYDFsDzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input = \"Hola me llamo Fernando, soy un programador\")"
      ],
      "metadata": {
        "id": "NPLw3v1KsaK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memoria.chat_memory.messages"
      ],
      "metadata": {
        "id": "bG0zKgLTsMam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input = \"Como me llamo?\")"
      ],
      "metadata": {
        "id": "BpEJ5TBwsJDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversation buffer window memory\n",
        "\n",
        "Esta memoria es igual a la anterior, con la diferencia que se puede definir una ventana de mensajes a recordar en vez de recordar todo el historico de interacciones."
      ],
      "metadata": {
        "id": "_rcxNUpRsPFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "YblBcF85sQOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI()\n",
        "memoria = ConversationBufferWindowMemory(k=2) #Numero de mensajes\n",
        "chatbot = ConversationChain(llm=llm, memory=memoria, verbose=True)"
      ],
      "metadata": {
        "id": "35ztjDrOsTLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input = \"Hola me llamo Fernando, soy un programador\")"
      ],
      "metadata": {
        "id": "FtITGzjetlEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input = \"Cuanto mide la Torre Latinoamericana\")"
      ],
      "metadata": {
        "id": "Jzaw8gbGt2DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input = \"Cuanto mide el Empire State\")"
      ],
      "metadata": {
        "id": "_zbj7Dh9t2Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memoria.chat_memory.messages"
      ],
      "metadata": {
        "id": "cnwMsHHgsTU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.predict(input = \"Como me llamo?\")"
      ],
      "metadata": {
        "id": "PcoL6cNcsTOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation summary memory\n",
        "\n",
        "Esta memoria en vez de almacenar un registro detallado de las interacciones, almacena un resumen de la conversaci칩n. Muy util para evitar prompts muy largos"
      ],
      "metadata": {
        "id": "kKYweaUUtoF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
        "#llm_k = OpenAI(stop=[\"\\nHuman\"])\n",
        "memoria = ConversationSummaryMemory(llm=llm)\n",
        "chatbot_resumen = ConversationChain(llm=llm, memory=memoria, verbose=True)"
      ],
      "metadata": {
        "id": "cw_IZk0Ltp-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_resumen.predict(input = \"Hola me llamo Fernando, soy un programador\")"
      ],
      "metadata": {
        "id": "jD5WRTyfulYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_resumen.predict(input = \"Me gusta la inteligencia artificial\")"
      ],
      "metadata": {
        "id": "G-JaZjAMuleZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_resumen.predict(input = \"Que te gusta a ti de la tecnologia?\")"
      ],
      "metadata": {
        "id": "DaIOqOeauHAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memoria.chat_memory.messages"
      ],
      "metadata": {
        "id": "kGJ_TaVHuIhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Knowledge Graph Memory"
      ],
      "metadata": {
        "id": "MQW3iVOau_ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationKGMemory\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "IsSI5kemvAmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
        "memoria = ConversationKGMemory(llm=llm)\n",
        "chatbot_kgm = ConversationChain(llm=llm, memory=memoria, verbose=True)"
      ],
      "metadata": {
        "id": "CBF-lE0BvE-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_kgm.predict(input = \"Hola me llamo Fernando, soy un programador\")"
      ],
      "metadata": {
        "id": "69N8YLOgvLHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatbot_kgm.memory.kg.get_triples())"
      ],
      "metadata": {
        "id": "BP18bTCRvTZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_kgm.predict(input = \"Mi perro se llama Juan\")"
      ],
      "metadata": {
        "id": "vgG0GsmOvU0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_kgm.predict(input = \"A que se dedica Alex?\")"
      ],
      "metadata": {
        "id": "xLxufIhFvGy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatbot_kgm.memory.kg.get_triples())"
      ],
      "metadata": {
        "id": "pFsnJEyBvIqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memoria.chat_memory.messages"
      ],
      "metadata": {
        "id": "ggauqOCxvKV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usando GPT3\n",
        "\n",
        "Las implementaciones que vimos son usando gpt3.5 turbo, pero si quieres hacerlo con gtp instruct como Davinci, Ada Curie o Babbage es muy similar"
      ],
      "metadata": {
        "id": "qf76lcE8vd5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "Tuv0CulLvfGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "lRLFsQeBvmJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Como me llamo?\")"
      ],
      "metadata": {
        "id": "XPqdaaatvn0y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}