{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp, OpenAI \n",
    "import config\n",
    "api = config.OPENAI_API_KEY "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm_openai = OpenAI(model_name = \"text-davinci-003\", openai_api_key=api)\n",
    "#llm_llama = LlamaCpp(model_path=\"./llamacpp/models/7B/ggml-model-q4_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_openai = llm_openai(\"Hola, como estas?\")\n",
    "#respuesta_llama = llm_llama(\"Hola, como estas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta_openai)\n",
    "#print(respuesta_llama)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chatgpt = ChatOpenAI(openai_api_key=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "respuesta = chatgpt([HumanMessage(content=\"Hola, como estas?\")])\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta\n",
    "#respuesta.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template_basico = \"\"\"Eres un asistente virtual culinario que responde a preguntas\n",
    "de manera muy breve.\n",
    "Pregunta: Cuales son los ingredientes para preparar {platillo}\n",
    "Respuesta:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp = PromptTemplate(input_variables=[\"platillo\"], template = template_basico)\n",
    "\n",
    "promt_value = prompt_temp.format(platillo=\"tacos al pastor\")\n",
    "print(promt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_openai = llm_openai(promt_value)\n",
    "print(respuesta_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai.get_num_tokens(promt_value) #Install tiktoken with pip install tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp_sistema = PromptTemplate(\n",
    "    template=\"Eres un asistente virtual que me recomienda una alternativa {adjetivo} a un producto\",\n",
    "    input_variables=[\"adjetivo\"],\n",
    ")\n",
    "\n",
    "template_sistema = SystemMessagePromptTemplate(prompt=prompt_temp_sistema)\n",
    "\n",
    "prompt_temp_humano = PromptTemplate(template=\"{texto}\", input_variables=[\"texto\"])\n",
    "\n",
    "template_humano = HumanMessagePromptTemplate(prompt=prompt_temp_humano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([template_sistema, template_humano])\n",
    "\n",
    "chat_promt_value = chat_prompt.format_prompt(adjetivo=\"economica\", texto=\"ipad\").to_messages()\n",
    "print(chat_promt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_resp = chatgpt(chat_promt_value)\n",
    "print(chat_resp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
