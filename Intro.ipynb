{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp, OpenAI \n",
    "import config\n",
    "api = config.OPENAI_API_KEY "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm_openai = OpenAI(model_name = \"text-davinci-003\", openai_api_key=api)\n",
    "#llm_llama = LlamaCpp(model_path=\"./llamacpp/models/7B/ggml-model-q4_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_openai = llm_openai(\"Hola, como estas?\")\n",
    "#respuesta_llama = llm_llama(\"Hola, como estas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta_openai)\n",
    "#print(respuesta_llama)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chatgpt = ChatOpenAI(openai_api_key=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "respuesta = chatgpt([HumanMessage(content=\"Hola, como estas?\")])\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta\n",
    "#respuesta.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template_basico = \"\"\"Eres un asistente virtual culinario que responde a preguntas\n",
    "de manera muy breve.\n",
    "Pregunta: Cuales son los ingredientes para preparar {platillo}\n",
    "Respuesta:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp = PromptTemplate(input_variables=[\"platillo\"], template = template_basico)\n",
    "\n",
    "promt_value = prompt_temp.format(platillo=\"tacos al pastor\")\n",
    "print(promt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_openai = llm_openai(promt_value)\n",
    "print(respuesta_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai.get_num_tokens(promt_value) #Install tiktoken with pip install tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp_sistema = PromptTemplate(\n",
    "    template=\"Eres un asistente virtual que me recomienda una alternativa {adjetivo} a un producto\",\n",
    "    input_variables=[\"adjetivo\"],\n",
    ")\n",
    "\n",
    "template_sistema = SystemMessagePromptTemplate(prompt=prompt_temp_sistema)\n",
    "\n",
    "prompt_temp_humano = PromptTemplate(template=\"{texto}\", input_variables=[\"texto\"])\n",
    "\n",
    "template_humano = HumanMessagePromptTemplate(prompt=prompt_temp_humano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([template_sistema, template_humano])\n",
    "\n",
    "chat_promt_value = chat_prompt.format_prompt(adjetivo=\"economica\", texto=\"ipad\").to_messages()\n",
    "print(chat_promt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_resp = chatgpt(chat_promt_value)\n",
    "print(chat_resp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "ejemplos = [\n",
    "    {\"pregunta\": \"¿Cuál es el ingrediente principal de la pizza?\", \"respuesta\": \"La masa y salsa de tomate\"},\n",
    "    {\"pregunta\": \"¿Cuál es el ingrediente principal de la hamburguesa?\", \"respuesta\": \"La carne y el pan\"},\n",
    "    {\"pregunta\": \"¿Cuál es el ingrediente principal del burrito?\", \"respuesta\": \"La tortilla y la carne\"}\n",
    "]\n",
    "\n",
    "promt_temp_ejemplos = PromptTemplate(input_variables=[\"pregunta\", \"respuesta\"], \n",
    "                                     template = \"Pregunta: {pregunta}\\nRespuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ejemplos = FewShotPromptTemplate(example_prompt=promt_temp_ejemplos, \n",
    "                                       examples=ejemplos, \n",
    "                                       prefix = \"Eres un asistenet virtual culinario que responde preguntas de manera muy breve\",\n",
    "                                       suffix = \"Pregunta: {pregunta}\\nRespuesta:\", \n",
    "                                        input_variables=[\"pregunta\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = promt_ejemplos.format(pregunta=\"¿Cuál es el ingrediente principal del coctel de camaron?\")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_ingredientes = llm_openai(prompt_value)\n",
    "print(respuesta_ingredientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_openai(\"¿Cuál es el ingrediente principal del coctel de camaron?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_openai.get_num_tokens(prompt_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_basico_parser = \"\"\"Cuales son los ingredientes para preparar {platillo}\\n{como_parsear}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp_parser = PromptTemplate(input_variables=[\"platillo\"], \n",
    "                                    template = template_basico_parser, \n",
    "                                    partial_variables={\"como_parsear\": format_instructions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "promt_value_parser = prompt_temp_parser.format(platillo=\"tacos al pastor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(promt_value_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_parser = llm_openai(promt_value_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(respuesta_parser)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
